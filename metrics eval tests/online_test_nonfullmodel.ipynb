{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Transformers and PyTorch\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, RobertaForSequenceClassification, RobertaTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# Sklearn for Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at AleOfDurin/Sentiment_fine_tuned and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"AleOfDurin/Sentiment_fine_tuned\"\n",
    "# Load the model and tokenizer from the Hugging Face Hub\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name) # Ensure it's SequenceClassification model  --- ,num_labels=2) deleted as it was added to the json file\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_282536\\861193737.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_encodings, val_labels = torch.load(r'C:\\Users\\aless\\Desktop\\final project 2.1\\val_encodings.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenized validation dataset (save the file to the path desired and link it here)\n",
    "val_encodings, val_labels = torch.load(r'C:\\Users\\aless\\Desktop\\final project 2.1\\val_encodings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and data moved to GPU, DataLoader set.\n"
     ]
    }
   ],
   "source": [
    "# Define the device: Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Set batch size and number of workers\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "\n",
    "# Create the validation DataLoader with batch size 512 and workers set to 8\n",
    "val_dataset = TensorDataset(\n",
    "    val_encodings['input_ids'].to(device),  # Move input IDs to GPU\n",
    "    val_encodings['attention_mask'].to(device),  # Move attention masks to GPU\n",
    "    val_labels.to(device)  # Move labels to GPU\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model and data moved to GPU, DataLoader set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.3),  # Dropout as per the trained model\n",
    "    torch.nn.Linear(model.config.hidden_size, 2)  # Linear layer for binary classification\n",
    ").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs: tensor([[    0, 12861,  7728,  2788,   259,     2],\n",
      "        [    0, 21518,  7728,  2788,     2,     1]], device='cuda:0')\n",
      "Tokenized Attention Mask: tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Example: Re-tokenizing the validation dataset (assuming you have the raw text data)\n",
    "texts = [\"Your sample text here\", \"Another sample text\"]  # Replace with your dataset's text\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Move tokenized inputs to the device\n",
    "input_ids = tokenized_inputs['input_ids'].to(device)\n",
    "attention_mask = tokenized_inputs['attention_mask'].to(device)\n",
    "\n",
    "# Debug: Print tokenized inputs\n",
    "print(f\"Tokenized Input IDs: {input_ids}\")\n",
    "print(f\"Tokenized Attention Mask: {attention_mask}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time: 568.10 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start timer for inference\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "y_proba = []  # Store probabilities for ROC-AUC\n",
    "\n",
    "# Set model to no_grad mode for inference\n",
    "with torch.no_grad():\n",
    "    # Loop through validation DataLoader\n",
    "    for step, batch in enumerate(val_loader):\n",
    "        # Move batch to the device (GPU or CPU)\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Check for NaN values in the input data\n",
    "        if torch.isnan(input_ids).any() or torch.isnan(attention_mask).any():\n",
    "            print(\"Input contains NaN values.\")\n",
    "            break  # Stop if NaNs are found\n",
    "        \n",
    "        # Remove mixed precision and run in full precision\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Use only the logits for the [CLS] token (first token) for classification\n",
    "        cls_logits = logits[:, 0, :]  # Extract the [CLS] token logits for each sequence\n",
    "        \n",
    "        # Predicted classes from [CLS] token\n",
    "        predictions = torch.argmax(cls_logits, dim=-1)\n",
    "        \n",
    "        # Probabilities for ROC-AUC from [CLS] token\n",
    "        probabilities = torch.softmax(cls_logits, dim=-1)[:, 1]  # Probabilities for class 1\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.extend(predictions.cpu().numpy())  # Store binary predictions for each sequence\n",
    "        all_labels.extend(labels.cpu().numpy())  # Store actual labels\n",
    "        y_proba.extend(probabilities.cpu().numpy())  # Store probabilities for ROC-AUC\n",
    "\n",
    "# Total time taken for inference\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total inference time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (all_preds): [0, 0, 0, 0, 0]\n",
      "Labels (all_labels): [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check the shape and some example values\n",
    "print(f\"Predictions (all_preds): {all_preds[:5]}\")  # Print first 5 predictions\n",
    "print(f\"Labels (all_labels): {all_labels[:5]}\")  # Print first 5 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Distribution: {0: 469723}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check distribution of predictions (if the model is predicting only one class)\n",
    "unique_preds, counts_preds = np.unique(all_preds, return_counts=True)\n",
    "pred_class_distribution = dict(zip(unique_preds, counts_preds))\n",
    "print(f\"Predicted Class Distribution: {pred_class_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_flat = np.concatenate([pred.flatten() for pred in all_preds])\n",
    "all_labels_flat = np.concatenate([label.flatten() for label in all_labels])\n",
    "y_proba_flat = np.array(y_proba).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    469723\n",
      "\n",
      "    accuracy                           1.00    469723\n",
      "   macro avg       1.00      1.00      1.00    469723\n",
      "weighted avg       1.00      1.00      1.00    469723\n",
      "\n",
      "Confusion Matrix:\n",
      "[[469723]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(conf_matrix)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ROC-AUC Score\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_proba_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC-AUC Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroc_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:619\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    617\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    618\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 619\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    622\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    623\u001b[0m ):\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_fpr \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_labels_flat, all_preds_flat)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate Classification Report (Precision, Recall, F1-Score)\n",
    "report = classification_report(all_labels_flat, all_preds_flat)\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels_flat, all_preds_flat)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# ROC-AUC Score\n",
    "roc_auc = roc_auc_score(all_labels_flat, y_proba_flat)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
