{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hf://datasets/PrkhrAwsti/Twitter_Sentiment_3M/twitter_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  sentiment\n",
       "0           0  is upset that he can't update his Facebook by ...        0.0\n",
       "1           1  @Kenichan I dived many times for the ball. Man...        0.0\n",
       "2           2    my whole body feels itchy and like its on fire         0.0\n",
       "3           3  @nationwideclass no, it's not behaving at all....        0.0\n",
       "4           4                      @Kwesidei not the whole crew         0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsample = df.sample(n=100,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsample.to_csv('sample_para_dashboard_prueba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.0223e-01, 4.5871e-01, 6.1152e-01],\n",
      "        [8.1092e-01, 3.1761e-02, 6.0722e-02],\n",
      "        [1.0787e-01, 4.2731e-04, 2.2336e-01],\n",
      "        [4.3608e-01, 5.2794e-01, 5.6576e-01],\n",
      "        [3.5234e-01, 5.1036e-01, 9.9494e-01]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_9612\\1183939198.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_encodings, val_labels = torch.load(r'C:/Users/aless/Desktop/final project 2.1/val_encodings.pt')\n",
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_9612\\1183939198.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time: 268.30 seconds\n",
      "Classification Report for Pre-trained RoBERTa Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67    235207\n",
      "           1       0.55      0.00      0.00    234516\n",
      "\n",
      "    accuracy                           0.50    469723\n",
      "   macro avg       0.53      0.50      0.33    469723\n",
      "weighted avg       0.53      0.50      0.34    469723\n",
      "\n",
      "ROC-AUC Score: 0.5246\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import RobertaForSequenceClassification\n",
    "from torch.cuda.amp import autocast\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import time\n",
    "\n",
    "# Step 1: Load the pre-trained RoBERTa model (without fine-tuned weights)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Step 2: Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Move the model to the same device (GPU or CPU) as your fine-tuned model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Step 4: Load the validation data (updated path)\n",
    "val_encodings, val_labels = torch.load(r'C:/Users/aless/Desktop/final project 2.1/val_encodings.pt')\n",
    "\n",
    "# Step 5: Prepare the validation DataLoader\n",
    "batch_size = 512\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "# Step 6: Evaluate the pre-trained model on the validation set\n",
    "# Initialize lists to store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "y_proba = []\n",
    "\n",
    "# Start timer for inference\n",
    "start_time = time.time()\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(val_loader):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Enable mixed precision for faster computation\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # No need to index for CLS token, logits directly predict the classes\n",
    "\n",
    "            # Predicted classes (0 for negative, 1 for positive)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            # Probabilities for ROC-AUC\n",
    "            probabilities = torch.softmax(logits, dim=-1)[:, 1]  # Probabilities for class 1\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        y_proba.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Total inference time\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total inference time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Step 7: Calculate classification report and ROC-AUC score\n",
    "print(\"Classification Report for Pre-trained RoBERTa Model:\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Step 8: Calculate ROC-AUC score\n",
    "roc_auc = roc_auc_score(all_labels, y_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
