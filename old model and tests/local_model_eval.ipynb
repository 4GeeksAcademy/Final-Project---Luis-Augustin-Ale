{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from safetensors.torch import load_file\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_path = r\"C:\\Users\\aless\\Desktop\\final project 2.1\\full_model\"\n",
    "model_path = r\"C:\\Users\\aless\\Desktop\\final project 2.1\\full_model\\model.safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(model_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the safetensors model using safetensors library (not torch.load)\n",
    "state_dict = load_file(model_path)  # Use safetensors to load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\aless\\Desktop\\final project 2.1\\full_model were not used when initializing RobertaForSequenceClassification: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\aless\\Desktop\\final project 2.1\\full_model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model using the folder containing the config and other files\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_folder_path,  # Path to folder, not the config file itself\n",
    "    state_dict=state_dict,  # Loaded safetensors weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_220112\\861193737.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_encodings, val_labels = torch.load(r'C:\\Users\\aless\\Desktop\\final project 2.1\\val_encodings.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenized validation dataset (save the file to the path desired and link it here)\n",
    "val_encodings, val_labels = torch.load(r'C:\\Users\\aless\\Desktop\\final project 2.1\\val_encodings.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and data moved to GPU, DataLoader set.\n"
     ]
    }
   ],
   "source": [
    "# Define the device: Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Set batch size and number of workers\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "\n",
    "# Create the validation DataLoader with batch size 512 and workers set to 8\n",
    "val_dataset = TensorDataset(\n",
    "    val_encodings['input_ids'].to(device),  # Move input IDs to GPU\n",
    "    val_encodings['attention_mask'].to(device),  # Move attention masks to GPU\n",
    "    val_labels.to(device)  # Move labels to GPU\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model and data moved to GPU, DataLoader set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.3),  # Dropout as per the trained model\n",
    "    torch.nn.Linear(model.config.hidden_size, 2)  # Linear layer for binary classification\n",
    ").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input IDs: tensor([[    0, 12861,  7728,  2788,   259,     2],\n",
      "        [    0, 21518,  7728,  2788,     2,     1]], device='cuda:0')\n",
      "Tokenized Attention Mask: tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Example: Re-tokenizing the validation dataset (assuming you have the raw text data)\n",
    "texts = [\"Your sample text here\", \"Another sample text\"]  # Replace with your dataset's text\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Move tokenized inputs to the device\n",
    "input_ids = tokenized_inputs['input_ids'].to(device)\n",
    "attention_mask = tokenized_inputs['attention_mask'].to(device)\n",
    "\n",
    "# Debug: Print tokenized inputs\n",
    "print(f\"Tokenized Input IDs: {input_ids}\")\n",
    "print(f\"Tokenized Attention Mask: {attention_mask}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the validation DataLoader\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=8)  # adding workers to imrpove cpu usage to avoid bottleneck (8 core = testing with 8 workers )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_220112\\3577330652.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "c:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time: 264.83 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.cuda.amp import autocast  # Import for mixed precision\n",
    "\n",
    "# Start timer for inference\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "y_proba = []  # Store probabilities for ROC-AUC\n",
    "\n",
    "# Set model to no_grad mode for inference\n",
    "with torch.no_grad():\n",
    "    # Loop through validation DataLoader\n",
    "    for step, batch in enumerate(val_loader):\n",
    "        # Move batch to the device (GPU or CPU)\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "        # Enable mixed precision for faster computation\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Use only the logits for the [CLS] token (first token) for classification\n",
    "            cls_logits = logits[:, 0, :]  # Extract the [CLS] token logits for each sequence\n",
    "            \n",
    "            # Predicted classes from [CLS] token\n",
    "            predictions = torch.argmax(cls_logits, dim=-1)\n",
    "            \n",
    "            # Probabilities for ROC-AUC from [CLS] token\n",
    "            probabilities = torch.softmax(cls_logits, dim=-1)[:, 1]  # Probabilities for class 1\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.extend(predictions.cpu().numpy())  # Store binary predictions for each sequence\n",
    "        all_labels.extend(labels.cpu().numpy())  # Store actual labels\n",
    "        y_proba.extend(probabilities.cpu().numpy())  # Store probabilities for ROC-AUC\n",
    "\n",
    "# Total time taken for inference\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total inference time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (all_preds): [1, 1, 1, 0, 1]\n",
      "Labels (all_labels): [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check the shape and some example values\n",
    "print(f\"Predictions (all_preds): {all_preds[:5]}\")  # Print first 5 predictions\n",
    "print(f\"Labels (all_labels): {all_labels[:5]}\")  # Print first 5 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions sample: [1, 1, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "True labels sample: [0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "Predicted Class Distribution: {0: 242483, 1: 227240}\n"
     ]
    }
   ],
   "source": [
    "# Print a few predictions and their corresponding labels\n",
    "print(\"Predictions sample:\", all_preds[:10])\n",
    "print(\"True labels sample:\", all_labels[:10])\n",
    "\n",
    "# Check distribution of predictions (if the model is predicting only one class)\n",
    "unique_preds, counts_preds = np.unique(all_preds, return_counts=True)\n",
    "pred_class_distribution = dict(zip(unique_preds, counts_preds))\n",
    "print(f\"Predicted Class Distribution: {pred_class_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with concat instead of flattened (after testing all 3 scenarios iwth flattening)  sane bad result.\n",
    "all_preds_concat = np.concatenate([np.atleast_1d(pred) for pred in all_preds])\n",
    "all_labels_concat = np.concatenate([np.atleast_1d(label) for label in all_labels])\n",
    "y_proba_concat = np.concatenate([np.atleast_1d(proba) for proba in y_proba])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.36      0.35    235207\n",
      "           1       0.34      0.33      0.33    234516\n",
      "\n",
      "    accuracy                           0.34    469723\n",
      "   macro avg       0.34      0.34      0.34    469723\n",
      "weighted avg       0.34      0.34      0.34    469723\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 84324 150883]\n",
      " [158159  76357]]\n",
      "ROC-AUC Score: 0.2810\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(all_labels_concat, all_preds_concat)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate Classification Report (Precision, Recall, F1-Score)\n",
    "report = classification_report(all_labels_concat, all_preds_concat)\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_labels_concat, all_preds_concat)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# ROC-AUC Score\n",
    "roc_auc = roc_auc_score(all_labels_concat, y_proba_concat)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
